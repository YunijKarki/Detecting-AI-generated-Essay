{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2TokenizerFast\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from IPython.display import FileLink, FileLinks\n",
    "import sklearn\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install datasets --upgrade            #resolving the error i.e keyerror: length when loading dataset from huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d09c0f96e9a4273bc69de3a6dafe306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d56f2505c3411ea5b0e93dfbc44c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/523M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf5031ffb9043e4bc53a0c4388e1d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce3755cf3574e16928e05a7b31ff7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb617dfebd344be3bf4e85cbf8fff420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenization and adding pad token (eos_token) especially for the text generational model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.padding_side = \"left\"\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# def tokenize_dataset(example):\n",
    "#     return tokenizer(example['text'], padding = 'max_length', truncation = True, max_length= 128, return_tensors = \"pt\")\n",
    "# tokenized_dataset = dataset.map(tokenize_dataset)\n",
    "# tokenized_dataset = tokenized_dataset.with_format('torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pushing to hugging face for future purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go into the paperspace terminal and type huggingface-cli login and enter the token and comeback here\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "# tokenized_dataset.push_to_hub('Yunij/daig2-tokenized-dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the updated tokenized dataset (padding to the left scenario after adding the eos token) to the huggingface hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d56a9c3db24da49e129a205349a187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/607 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e0bba0285e4561bac2d5eaf82bfc5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/109M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7232b7192c4429cb4a5f78e8cf1a399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/44868 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = load_dataset('Yunij/daig2-tokenized-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_dataset = tokenized_dataset['train'].train_test_split(test_size=0.2, seed = 42) \n",
    "tokenized_dataset = tokenized_dataset.with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'prompt_name', 'source', 'RDizzl3_seven', 'cleaned_text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 35894\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'prompt_name', 'source', 'RDizzl3_seven', 'cleaned_text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 8974\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your model (finetuning on top of gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class gpt2essayprediction(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes: int, hidden_size: int, max_sequence_length: int, model_id: str):\n",
    "        super().__init__()  \n",
    "        self.gpt2model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "        self.linear_projection = nn.Linear(hidden_size * max_sequence_length, num_classes)   #input should be matched\n",
    "\n",
    "    def forward(self, input_id, attention_mask):\n",
    "        # input_dict = {'input_id': input_id,'attention_mask': attention_mask}\n",
    "\n",
    "        logits,_ = self.gpt2model(input_ids = input_id, attention_mask = attention_mask, return_dict = False)\n",
    "        batch_size = logits.shape[0]\n",
    "        output = self.linear_projection(logits.view(batch_size,-1))   #converting (16,columns) to (16,2) where 2 is the num_classes\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global values\n",
    "\n",
    "hidden_size = 50257\n",
    "max_sequence_length = 128\n",
    "learning_rate = 3e-4\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = gpt2essayprediction(num_classes=2, hidden_size=hidden_size, max_sequence_length=max_sequence_length, model_id=\"gpt2\").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function and creating a pytorch optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating pytorch training and testing Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = tokenized_dataset[\"train\"]\n",
    "testing_data = tokenized_dataset[\"test\"]\n",
    "train_dataloader = DataLoader(\n",
    "    training_data, batch_size=16, shuffle=True, drop_last=True\n",
    ")\n",
    "test_dataloader = DataLoader(testing_data, batch_size=16, shuffle=True, drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2243/2243 [05:18<00:00,  7.03it/s]\n",
      "100%|██████████| 560/560 [00:30<00:00, 18.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  21.92564         | Val Loss:  1.33507         | train_accuracy:  0.90494         | validation_accuracy:  0.96958         | Roc_score:  1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2243/2243 [05:16<00:00,  7.10it/s]\n",
      "100%|██████████| 560/560 [00:30<00:00, 18.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.73277         | Val Loss:  0.69925         | train_accuracy:  0.97286         | validation_accuracy:  0.98039         | Roc_score:  0.95000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 226/2243 [00:31<04:42,  7.15it/s]"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "    total_acc_train = 0\n",
    "    total_acc_val = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_index, train_input in enumerate(tqdm(train_dataloader)):\n",
    "        label = train_input[\"label\"].to(device)\n",
    "\n",
    "        input_ids = train_input[\"input_ids\"].squeeze(1).to(device)\n",
    "        attention_mask = (\n",
    "            train_input[\"attention_mask\"].squeeze(1).to(device)\n",
    "        )  # removing the unnecessary dimension by squeezing it out\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(logits, label)\n",
    "        total_train_loss += loss.item()\n",
    "        acc = (logits.argmax(dim=1) == label).sum().item()\n",
    "        total_acc_train += acc\n",
    "\n",
    "        # backpropagation\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, test_input in enumerate(tqdm(test_dataloader)):\n",
    "            label = test_input[\"label\"].to(device)\n",
    "            input_ids = test_input[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask = (\n",
    "                test_input[\"attention_mask\"].squeeze(1).to(device)\n",
    "            )  # removing the unnecessary dimension by squeezing it out\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(logits, label)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            # accuracy_score\n",
    "            acc = (logits.argmax(dim=1) == label).sum().item()\n",
    "            total_acc_val += acc\n",
    "\n",
    "            # roc_score\n",
    "\n",
    "            predictions = torch.argmax(logits.cpu(), dim=1)\n",
    "            try:\n",
    "                roc_score = roc_auc_score(\n",
    "                    predictions, label.cpu()\n",
    "                )  # catching the exception like ValueError\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    print(\n",
    "        f\"Epochs: {epoch + 1} | Train Loss: {total_train_loss/len(training_data): .5f} \\\n",
    "        | Val Loss: {total_val_loss / len(testing_data): .5f} \\\n",
    "        | train_accuracy: {total_acc_train/len(training_data): .5f} \\\n",
    "        | validation_accuracy: {total_acc_val/len(testing_data): .5f} \\\n",
    "        | Roc_score: {roc_score: .5f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state dict keys: \n",
      "\n",
      " odict_keys(['gpt2model.transformer.wte.weight', 'gpt2model.transformer.wpe.weight', 'gpt2model.transformer.h.0.ln_1.weight', 'gpt2model.transformer.h.0.ln_1.bias', 'gpt2model.transformer.h.0.attn.bias', 'gpt2model.transformer.h.0.attn.masked_bias', 'gpt2model.transformer.h.0.attn.c_attn.weight', 'gpt2model.transformer.h.0.attn.c_attn.bias', 'gpt2model.transformer.h.0.attn.c_proj.weight', 'gpt2model.transformer.h.0.attn.c_proj.bias', 'gpt2model.transformer.h.0.ln_2.weight', 'gpt2model.transformer.h.0.ln_2.bias', 'gpt2model.transformer.h.0.mlp.c_fc.weight', 'gpt2model.transformer.h.0.mlp.c_fc.bias', 'gpt2model.transformer.h.0.mlp.c_proj.weight', 'gpt2model.transformer.h.0.mlp.c_proj.bias', 'gpt2model.transformer.h.1.ln_1.weight', 'gpt2model.transformer.h.1.ln_1.bias', 'gpt2model.transformer.h.1.attn.bias', 'gpt2model.transformer.h.1.attn.masked_bias', 'gpt2model.transformer.h.1.attn.c_attn.weight', 'gpt2model.transformer.h.1.attn.c_attn.bias', 'gpt2model.transformer.h.1.attn.c_proj.weight', 'gpt2model.transformer.h.1.attn.c_proj.bias', 'gpt2model.transformer.h.1.ln_2.weight', 'gpt2model.transformer.h.1.ln_2.bias', 'gpt2model.transformer.h.1.mlp.c_fc.weight', 'gpt2model.transformer.h.1.mlp.c_fc.bias', 'gpt2model.transformer.h.1.mlp.c_proj.weight', 'gpt2model.transformer.h.1.mlp.c_proj.bias', 'gpt2model.transformer.h.2.ln_1.weight', 'gpt2model.transformer.h.2.ln_1.bias', 'gpt2model.transformer.h.2.attn.bias', 'gpt2model.transformer.h.2.attn.masked_bias', 'gpt2model.transformer.h.2.attn.c_attn.weight', 'gpt2model.transformer.h.2.attn.c_attn.bias', 'gpt2model.transformer.h.2.attn.c_proj.weight', 'gpt2model.transformer.h.2.attn.c_proj.bias', 'gpt2model.transformer.h.2.ln_2.weight', 'gpt2model.transformer.h.2.ln_2.bias', 'gpt2model.transformer.h.2.mlp.c_fc.weight', 'gpt2model.transformer.h.2.mlp.c_fc.bias', 'gpt2model.transformer.h.2.mlp.c_proj.weight', 'gpt2model.transformer.h.2.mlp.c_proj.bias', 'gpt2model.transformer.h.3.ln_1.weight', 'gpt2model.transformer.h.3.ln_1.bias', 'gpt2model.transformer.h.3.attn.bias', 'gpt2model.transformer.h.3.attn.masked_bias', 'gpt2model.transformer.h.3.attn.c_attn.weight', 'gpt2model.transformer.h.3.attn.c_attn.bias', 'gpt2model.transformer.h.3.attn.c_proj.weight', 'gpt2model.transformer.h.3.attn.c_proj.bias', 'gpt2model.transformer.h.3.ln_2.weight', 'gpt2model.transformer.h.3.ln_2.bias', 'gpt2model.transformer.h.3.mlp.c_fc.weight', 'gpt2model.transformer.h.3.mlp.c_fc.bias', 'gpt2model.transformer.h.3.mlp.c_proj.weight', 'gpt2model.transformer.h.3.mlp.c_proj.bias', 'gpt2model.transformer.h.4.ln_1.weight', 'gpt2model.transformer.h.4.ln_1.bias', 'gpt2model.transformer.h.4.attn.bias', 'gpt2model.transformer.h.4.attn.masked_bias', 'gpt2model.transformer.h.4.attn.c_attn.weight', 'gpt2model.transformer.h.4.attn.c_attn.bias', 'gpt2model.transformer.h.4.attn.c_proj.weight', 'gpt2model.transformer.h.4.attn.c_proj.bias', 'gpt2model.transformer.h.4.ln_2.weight', 'gpt2model.transformer.h.4.ln_2.bias', 'gpt2model.transformer.h.4.mlp.c_fc.weight', 'gpt2model.transformer.h.4.mlp.c_fc.bias', 'gpt2model.transformer.h.4.mlp.c_proj.weight', 'gpt2model.transformer.h.4.mlp.c_proj.bias', 'gpt2model.transformer.h.5.ln_1.weight', 'gpt2model.transformer.h.5.ln_1.bias', 'gpt2model.transformer.h.5.attn.bias', 'gpt2model.transformer.h.5.attn.masked_bias', 'gpt2model.transformer.h.5.attn.c_attn.weight', 'gpt2model.transformer.h.5.attn.c_attn.bias', 'gpt2model.transformer.h.5.attn.c_proj.weight', 'gpt2model.transformer.h.5.attn.c_proj.bias', 'gpt2model.transformer.h.5.ln_2.weight', 'gpt2model.transformer.h.5.ln_2.bias', 'gpt2model.transformer.h.5.mlp.c_fc.weight', 'gpt2model.transformer.h.5.mlp.c_fc.bias', 'gpt2model.transformer.h.5.mlp.c_proj.weight', 'gpt2model.transformer.h.5.mlp.c_proj.bias', 'gpt2model.transformer.h.6.ln_1.weight', 'gpt2model.transformer.h.6.ln_1.bias', 'gpt2model.transformer.h.6.attn.bias', 'gpt2model.transformer.h.6.attn.masked_bias', 'gpt2model.transformer.h.6.attn.c_attn.weight', 'gpt2model.transformer.h.6.attn.c_attn.bias', 'gpt2model.transformer.h.6.attn.c_proj.weight', 'gpt2model.transformer.h.6.attn.c_proj.bias', 'gpt2model.transformer.h.6.ln_2.weight', 'gpt2model.transformer.h.6.ln_2.bias', 'gpt2model.transformer.h.6.mlp.c_fc.weight', 'gpt2model.transformer.h.6.mlp.c_fc.bias', 'gpt2model.transformer.h.6.mlp.c_proj.weight', 'gpt2model.transformer.h.6.mlp.c_proj.bias', 'gpt2model.transformer.h.7.ln_1.weight', 'gpt2model.transformer.h.7.ln_1.bias', 'gpt2model.transformer.h.7.attn.bias', 'gpt2model.transformer.h.7.attn.masked_bias', 'gpt2model.transformer.h.7.attn.c_attn.weight', 'gpt2model.transformer.h.7.attn.c_attn.bias', 'gpt2model.transformer.h.7.attn.c_proj.weight', 'gpt2model.transformer.h.7.attn.c_proj.bias', 'gpt2model.transformer.h.7.ln_2.weight', 'gpt2model.transformer.h.7.ln_2.bias', 'gpt2model.transformer.h.7.mlp.c_fc.weight', 'gpt2model.transformer.h.7.mlp.c_fc.bias', 'gpt2model.transformer.h.7.mlp.c_proj.weight', 'gpt2model.transformer.h.7.mlp.c_proj.bias', 'gpt2model.transformer.h.8.ln_1.weight', 'gpt2model.transformer.h.8.ln_1.bias', 'gpt2model.transformer.h.8.attn.bias', 'gpt2model.transformer.h.8.attn.masked_bias', 'gpt2model.transformer.h.8.attn.c_attn.weight', 'gpt2model.transformer.h.8.attn.c_attn.bias', 'gpt2model.transformer.h.8.attn.c_proj.weight', 'gpt2model.transformer.h.8.attn.c_proj.bias', 'gpt2model.transformer.h.8.ln_2.weight', 'gpt2model.transformer.h.8.ln_2.bias', 'gpt2model.transformer.h.8.mlp.c_fc.weight', 'gpt2model.transformer.h.8.mlp.c_fc.bias', 'gpt2model.transformer.h.8.mlp.c_proj.weight', 'gpt2model.transformer.h.8.mlp.c_proj.bias', 'gpt2model.transformer.h.9.ln_1.weight', 'gpt2model.transformer.h.9.ln_1.bias', 'gpt2model.transformer.h.9.attn.bias', 'gpt2model.transformer.h.9.attn.masked_bias', 'gpt2model.transformer.h.9.attn.c_attn.weight', 'gpt2model.transformer.h.9.attn.c_attn.bias', 'gpt2model.transformer.h.9.attn.c_proj.weight', 'gpt2model.transformer.h.9.attn.c_proj.bias', 'gpt2model.transformer.h.9.ln_2.weight', 'gpt2model.transformer.h.9.ln_2.bias', 'gpt2model.transformer.h.9.mlp.c_fc.weight', 'gpt2model.transformer.h.9.mlp.c_fc.bias', 'gpt2model.transformer.h.9.mlp.c_proj.weight', 'gpt2model.transformer.h.9.mlp.c_proj.bias', 'gpt2model.transformer.h.10.ln_1.weight', 'gpt2model.transformer.h.10.ln_1.bias', 'gpt2model.transformer.h.10.attn.bias', 'gpt2model.transformer.h.10.attn.masked_bias', 'gpt2model.transformer.h.10.attn.c_attn.weight', 'gpt2model.transformer.h.10.attn.c_attn.bias', 'gpt2model.transformer.h.10.attn.c_proj.weight', 'gpt2model.transformer.h.10.attn.c_proj.bias', 'gpt2model.transformer.h.10.ln_2.weight', 'gpt2model.transformer.h.10.ln_2.bias', 'gpt2model.transformer.h.10.mlp.c_fc.weight', 'gpt2model.transformer.h.10.mlp.c_fc.bias', 'gpt2model.transformer.h.10.mlp.c_proj.weight', 'gpt2model.transformer.h.10.mlp.c_proj.bias', 'gpt2model.transformer.h.11.ln_1.weight', 'gpt2model.transformer.h.11.ln_1.bias', 'gpt2model.transformer.h.11.attn.bias', 'gpt2model.transformer.h.11.attn.masked_bias', 'gpt2model.transformer.h.11.attn.c_attn.weight', 'gpt2model.transformer.h.11.attn.c_attn.bias', 'gpt2model.transformer.h.11.attn.c_proj.weight', 'gpt2model.transformer.h.11.attn.c_proj.bias', 'gpt2model.transformer.h.11.ln_2.weight', 'gpt2model.transformer.h.11.ln_2.bias', 'gpt2model.transformer.h.11.mlp.c_fc.weight', 'gpt2model.transformer.h.11.mlp.c_fc.bias', 'gpt2model.transformer.h.11.mlp.c_proj.weight', 'gpt2model.transformer.h.11.mlp.c_proj.bias', 'gpt2model.transformer.ln_f.weight', 'gpt2model.transformer.ln_f.bias', 'gpt2model.lm_head.weight', 'linear_projection.weight', 'linear_projection.bias'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to reconstruct the model exactly as it was when trained at loading time, So we need to store information about the model architecture in the checkpoint, along with the state dict.\n",
    "\n",
    "If you are planning to continue training of the model you'll need to store the optimizer state too.\n",
    "\n",
    "To do this, you build a dictionary with all the information you need to compeletely rebuild the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'model': gpt2essayprediction(num_classes = 2, hidden_size=50257,max_sequence_length=128, model_id = 'gpt2'),\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, '/notebooks/checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the saved checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/notebooks/checkpoint.pth'\n",
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = checkpoint['model']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 16\n",
    "# test_dataset = tokenized_dataset['test']\n",
    "# total = len(test_dataset)\n",
    "\n",
    "# for i in range(0,total,batch_size):\n",
    "\n",
    "#     input_ids = test_dataset['input_ids'].squeeze(1)\n",
    "#     attention_mask = test_dataset['attention_mask'].squeeze(1)\n",
    "#     batch_input_ids = input_ids[i:i + batch_size]\n",
    "#     batch_attention_mask = attention_mask[i: i + batch_size]\n",
    "#     # input_dict = {'input_ids': batch_input_ids,\n",
    "#     #               'attention_mask': batch_attention_mask\n",
    "#     # }\n",
    "#     with torch.no_grad():\n",
    "#         logits, _= model(input_ids = batch_input_ids, attention_mask = batch_attention_mask, return_dict = False) #input_format....return_dict = False for returning logits as a torch.tensor types.\n",
    "#     break \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
